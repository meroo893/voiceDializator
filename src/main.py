# -*- coding: utf-8 -*-
"""voice_extract.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1661y-iFFFaLeMDaSKikrxmmZqnZg-tmt
"""
import os
os.mkdir('../data/separated')

"""
REQUIREMENTS:

!pip install pydub
!pip install pytorch==2.0.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia
!pip install whisperx
!apt-get install libcudnn8 #VAJNO ZA DA NE KRASHWA
"""


import torch
print(torch.cuda.is_available())  # Must return True, either we've got a problem
print(torch.version.cuda)

import whisperx
device = "cuda"
audio_file = "inpt.wav"
batch_size = 16 # reduce if low on GPU mem
compute_type = "float16" # change to "int8" if low on GPU mem (may reduce accuracy)
model = whisperx.load_model("large-v3", device, compute_type=compute_type)


audio = whisperx.load_audio(audio_file)
result = model.transcribe(audio, batch_size=batch_size)
print(result["segments"]) # before alignment

align_model = "infinitejoy/wav2vec2-large-xls-r-300m-bulgarian"  # Change if you find a better model
l_a, metadata = whisperx.load_align_model(language_code=result["language"],
                                          device=device,
                                          model_name=align_model)

result = whisperx.align(result["segments"], l_a, metadata, audio, device, return_char_alignments=False)

print(result["segments"])  # after alignment

from pyannote.audio.pipelines import SpeakerDiarization
from pyannote.audio import Model, Inference
from huggingface_hub import login
from google.colab import userdata
HF_SECRET = userdata.get('HF_SECRET')

login(token=HF_SECRET)  # Authenticate / HF_SECRET is my huggingface API key

# 3. Assign speaker labels
diarize_model = whisperx.DiarizationPipeline(device=device, )

# add min/max number of speakers if known
diarize_segments = diarize_model(audio, min_speakers=2, max_speakers=2)

# diarize_segments = diarize_model(audio
# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)

result = whisperx.assign_word_speakers(diarize_segments, result)
print(diarize_segments)
print(result["segments"]) # segments are now assigned speaker IDs

SAVE_FOLDER = "labeled_voices"

from pydub import AudioSegment

speaker_mapping ={
    "SPEAKER_00": "Cvetanka_Rizova",
    "SPEAKER_01": "Kiril_Petkov",
}

# Load full audio file
audio = AudioSegment.from_file(audio_file)  # Automatically detects format (MP3, WAV, etc.)

# Loop through diarized segments and save each one as a WAV file
for i, segment in enumerate(result["segments"]):
    start_time = segment["start"] * 1000  # Convert to milliseconds
    end_time = segment["end"] * 1000      # Convert to milliseconds
    speaker_id = segment.get("speaker", "unknown")  # Get speaker ID
    speaker = speaker_mapping.get(speaker_id, speaker_id)  # Use mapping if available

    # Extract segment
    segment_audio = audio[start_time:end_time]

    # Define file name
    segment_filename = f"speaker_{speaker}_segment_{i}.wav"

    # Export as WAV
    segment_audio.export(f"{SAVE_FOLDER}/{segment_filename}", format="wav")

    print(f"Saved: {segment_filename}")